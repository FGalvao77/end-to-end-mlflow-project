Metadata-Version: 2.4
Name: end-to-end-mlflow-project
Version: 0.1.0
Summary: Example end-to-end MLflow project with training, serving and monitoring.
Author: Fernando Galv√£o
Project-URL: Source, https://github.com/FGalvao77/end-to-end-mlflow-project
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"

# üöÄ End-to-End MLflow Project

![Python](https://img.shields.io/badge/Python-3.9%2B-blue?logo=python&logoColor=white)
![MLflow](https://img.shields.io/badge/MLOps-MLflow-orange?logo=mlflow)
![GitHub Actions](https://img.shields.io/badge/CI/CD-GitHub%20Actions-lightgrey?logo=githubactions&logoColor=blue)
![Lint](https://img.shields.io/badge/Lint-Flake8-yellow?logo=python)
![Tests](https://img.shields.io/badge/Tests-Pytest-green?logo=pytest)
![License](https://img.shields.io/badge/License-MIT-success?logo=open-source-initiative)

---

## üìñ Vis√£o Geral (PT-BR)
Este projeto demonstra a constru√ß√£o de um pipeline **end-to-end de Machine Learning** com **MLflow**, cobrindo todas as etapas de **MLOps**:  
- Ingest√£o e pr√©-processamento de dados  
- Treinamento e rastreamento de experimentos  
- Avalia√ß√£o e sele√ß√£o de modelos  
- Deployment em ambiente produtivo  

O objetivo √© servir como um **cart√£o de visitas t√©cnico**, mostrando experi√™ncia pr√°tica em **engenharia de machine learning aplicada a produ√ß√£o**.

---

## üìñ Overview (EN)
This project demonstrates the implementation of an **end-to-end Machine Learning pipeline** using **MLflow**, covering all **MLOps** stages:  
- Data ingestion and preprocessing  
- Model training and experiment tracking  
- Model evaluation and selection  
- Deployment into production environment  

The goal is to serve as a **technical portfolio project**, showcasing practical experience in **machine learning engineering applied to production**.

---

## üèóÔ∏è Arquitetura / Architecture
Estrutura de pastas do projeto ap√≥s reorganiza√ß√£o:

```
end-to-end-mlflow-project/
‚îÇ‚îÄ‚îÄ docs/                  # documenta√ß√£o, guias e cheatsheets
‚îÇ   ‚îÇ‚îÄ‚îÄ fastapi/           # API-related docs (endpoints, cheatsheet, summary)
‚îÇ   ‚îÇ‚îÄ‚îÄ monitoring/        # Prometheus/Grafana material, quick-start
‚îÇ   ‚îÇ‚îÄ‚îÄ usage/             # passo a passo / instructions
‚îÇ   ‚îÇ‚îÄ‚îÄ MLFLOW_SOLUTION.md # arquitetura geral e notas
‚îÇ‚îÄ‚îÄ notebooks/             # Jupyter notebooks e experiment notes
‚îÇ‚îÄ‚îÄ src/                   # c√≥digo-fonte do pacote Python
‚îÇ   ‚îî‚îÄ‚îÄ mlops_project/
‚îÇ       ‚îú‚îÄ‚îÄ api.py
‚îÇ       ‚îú‚îÄ‚îÄ train.py
‚îÇ       ‚îú‚îÄ‚îÄ evaluate.py
‚îÇ       ‚îú‚îÄ‚îÄ deploy.py
‚îÇ       ‚îú‚îÄ‚îÄ utils.py
‚îÇ       ‚îî‚îÄ‚îÄ ...            # demais m√≥dulos
‚îÇ‚îÄ‚îÄ tests/                 # suite de testes pytest
‚îÇ‚îÄ‚îÄ k8s/                   # manifests de Kubernetes
‚îÇ‚îÄ‚îÄ scripts/               # utilit√°rios de shell
‚îÇ‚îÄ‚îÄ requirements.txt       # lista de depend√™ncias (pip)
‚îÇ‚îÄ‚îÄ pyproject.toml         # configura√ß√µes de empacotamento
‚îÇ‚îÄ‚îÄ README.md              # documenta√ß√£o principal
‚îÇ‚îÄ‚îÄ .gitignore             # arquivos ignorados pelo Git
```

A conven√ß√£o `src/` garante que o pacote `mlops_project` seja instalado via `pip install -e .` e evita problemas com importa√ß√£o relativa em testes.

---

## ‚öôÔ∏è Tecnologias / Technologies
- **Python 3.9+**  
- **MLflow** (Tracking, Models, UI)  
- **Git + GitHub** for version control  
- Modular structure following engineering best practices  

---

## üî¨ MLflow ‚Äì Integra√ß√£o / Integration
O projeto utiliza **MLflow Tracking** e **MLflow Models** para:  
The project uses **MLflow Tracking** and **MLflow Models** for:  
- Registro autom√°tico de par√¢metros, m√©tricas e artefatos / Automatic logging of parameters, metrics, and artifacts  
- Padroniza√ß√£o de modelos para deployment / Standardized model packaging for deployment  
- Compara√ß√£o de experimentos via MLflow UI / Experiment comparison via MLflow UI  

Exemplo / Example:
```python
import mlflow

with mlflow.start_run():
    mlflow.log_param('model_type', 'RandomForest')
    mlflow.log_metric('accuracy', 0.92)
    mlflow.sklearn.log_model(model, 'model')
```

---

## ‚ñ∂Ô∏è Como Executar / How to Run

### Instala√ß√£o / Installation
```bash
# create a Python virtual environment and activate it
python -m venv .venv
source .venv/bin/activate

# clone repository and install dependencies
git clone https://github.com/FGalvao77/end-to-end-mlflow-project.git
cd end-to-end-mlflow-project

# install package in editable mode and then dependencies
pip install -e .
pip install -r requirements.txt
# the `-e` flag makes `mlops_project` importable by tests and scripts
``` 

> üìÅ toda a documenta√ß√£o adicional foi movida para a pasta `docs/` (Ex.: `docs/usage/`).

### Pipeline
```bash
# run project modules using the installed package or by invoking the module path
python -m mlops_project.data_preprocessing    # data preparation
python -m mlops_project.train                 # training and logging
python -m mlops_project.evaluate              # evaluation reports
python -m mlops_project.deploy                # model export / packaging
```

### Running the test suite
The repository includes simple unit tests for training, evaluation and
deployment logic.  To execute them (from the project root) run:

```bash
.venv/bin/python -m pytest -q
```

The helper `train.train_model()` used by the tests sets up an isolated
file‚Äëbased tracking store so the tests never require an MLflow server and
don't leave garbage behind.

### MLflow UI
By default the training script uses a **file-based tracking store** located
inside the project (`mlruns/`).  This means you can run the pipeline without
having an MLflow server running and you won't see any connection errors
(see `src/mlops_project/train.py` for details).

If you prefer to run an MLflow server inside Docker you can use the
provided compose file.  Start it after you have activated your virtual
environment:

```bash
export DOCKER_API_VERSION=1.44
docker compose -f src/mlops_project/docker-compose.mlflow.yml up -d
```

The UI (when started via the provided compose file) will be available at **`http://127.0.0.1:5001`** (NOT `http://0.0.0.0:5000` 
which doesn't work in browsers). You can also point the code at a remote 
tracking server by setting `MLFLOW_TRACKING_URI` in your environment:

```bash
# Access MLflow UI in browser (compose mapping uses host port 5001):
http://127.0.0.1:5001        # ‚úì Localhost (recommended)
http://localhost:5001        # ‚úì Hostname alias

# Point training code to MLflow server (from host, when using compose):
export MLFLOW_TRACKING_URI=http://127.0.0.1:5001  # From host
# OR inside container:
export MLFLOW_TRACKING_URI=http://mlflow:5000     # Docker network
```

If you prefer to use the standalone server or view experiments through the
web UI, start it in the project root before running `train.py`:

```bash
mlflow ui
```

The UI will be available at `http://127.0.0.1:5000` unless you override the
port.  You can also point the code at a remote tracking server by setting
`MLFLOW_TRACKING_URI` in your environment, e.g.: 

```bash
export MLFLOW_TRACKING_URI=http://localhost:5000
```

### Handling corrupted tracking stores
The training script now automatically cleans up any non‚Äënumeric subdirectories
under `mlruns/` at startup ‚Äì these are usually created when the
`MLFLOW_ARTIFACT_URI` was mis‚Äëconfigured and they can lead to warnings or
failures such as the ``MissingConfigException`` shown above.  If you ever
experience errors during MLflow logging you can safely remove the entire
``mlruns/`` directory:

```bash
rm -rf mlruns
```

Additionally the script prints warnings and continues if it is unable to log a
model (e.g. due to a missing ``meta.yaml`` or permission error), so training
itself will not abort just because of MLflow issues.

> **Tip:** you can inspect the output for messages prefixed with ``WARNING:``
> to understand why MLflow logging may have failed; these will not stop the
> pipeline from producing artifacts in the `artifacts/` folder.

### Running Training with Remote MLflow Server

When you run training against a **remote MLflow server** (HTTP), you may encounter
a `Permission denied: '/mlruns'` error. This happens because:

1. The client (training script running on the **host**) tries to write artifacts directly to paths like `/mlruns`
2. The MLflow server runs in a **Docker container** and `/mlruns` is a container-internal path
3. The host OS cannot access container-internal paths

#### Solution 1: Run training INSIDE the Docker container (Recommended)

Run training from within the container where paths are consistent:

```bash
# Start the MLflow server first
export DOCKER_API_VERSION=1.44
docker compose -f src/mlops_project/docker-compose.mlflow.yml up -d mlflow

# Wait for server to be ready (30-40 seconds) then run training inside container
docker compose -f src/mlops_project/docker-compose.mlflow.yml exec training python train.py
```

#### Solution 2: Use file-based tracking (No server required)

Simply don't set `MLFLOW_TRACKING_URI` - the script defaults to a local file store:

```bash
# Just run training without server
.venv/bin/python src/mlops_project/train.py

# View experiments in the UI
mlflow ui

# Se houver mensagem de erro, como:

ERROR: [Errno 98] Adress already in use

# 1‚Å∞ Verifique que processo est√° usando o 5000 (ou a porta que voc√™ especificou):
lsof -i :5000
# ou
ss -ltnp | grep 5000

# 2‚Å∞ Mate o processo (no exemplo abaixo o PID √© 12345):
kill 12345
# se n√£o encerrar:
kill -9 12345

# 3‚Å∞ Reinicie a UI opcionalmente indicando outra porta se preferir:
mlflow ui --port 5001
```

### Running the built image (examples)

Start the MLflow tracking server from the image (host port 5001 -> container 5000):

```bash
# start mlflow server
docker run --rm --name mlflow_local -p 5001:5000 \
    -v "$(pwd)/mlruns":/mlruns:rw \
    -e MLFLOW_BACKEND_STORE_URI=sqlite:////mlruns/mlflow.db \
    -e MLFLOW_DEFAULT_ARTIFACT_ROOT=file:///mlruns/artifacts \
    my-mlflow-app:latest
```

Serve a saved MLflow model (host 8000 -> container 8000). Set `SERVE_MODEL=1` and `SERVE_MODEL_PATH`:

```bash
# serve model from project artifacts
docker run --rm --name model_server -p 8000:8000 \
    -v "$(pwd)/artifacts/model":/model:ro \
    -e SERVE_MODEL=1 -e SERVE_MODEL_PATH=/model \
    my-mlflow-app:latest
```

Check endpoints:

```bash
curl -s http://127.0.0.1:5001/health   # MLflow server
curl -s http://127.0.0.1:8000/ping     # model server
```

### Deploying to Kubernetes

The project includes Kubernetes manifests for production-ready deployment:

```bash
# Deploy to Kubernetes cluster (default: Minikube)
kubectl apply -f k8s/mlflow-deployment.yaml

# Check status
kubectl get all -n mlflow-prod

# For detailed K8s setup, troubleshooting, and CI/CD integration see k8s/README.md
```

**Quick Kubernetes Commands:**
```bash
# Port-forward for local access (Minikube)
kubectl port-forward svc/mlflow-service -n mlflow-prod 5000:5000 &
kubectl port-forward svc/model-service -n mlflow-prod 8000:8000 &

# View MLflow UI: http://localhost:5000
# Model Server: http://localhost:8000/ping

# Scale model server
kubectl scale deployment model-server -n mlflow-prod --replicas=5

# Remove all K8s resources
kubectl delete namespace mlflow-prod
```

See [k8s/README.md](k8s/README.md) for:
- Full installation guide (kubectl, Minikube, cloud clusters)  
- Image registry setup (Docker Hub, ECR, GCR)  
- Monitoring, scaling, and troubleshooting  
- CI/CD automation examples  

#### Solution 3: Configure S3-compatible artifact store (Production)

For production, use a cloud artifact store (S3, MinIO, etc.) instead of file system:

```bash
export MLFLOW_TRACKING_URI=http://127.0.0.1:5000
export MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://my-bucket/mlflow-artifacts

# Or for MinIO (S3-compatible):
export MLFLOW_ARTIFACT_ROOT_BUCKET=mlflow
export AWS_ACCESS_KEY_ID=minioadmin
export AWS_SECRET_ACCESS_KEY=minioadmin

.venv/bin/python src/mlops_project/train.py
```

---

## üìä Monitoramento e Observabilidade / Monitoring & Observability

### Prometheus + Grafana Stack

This project includes a complete monitoring solution using **Prometheus** and **Grafana** for observability of MLflow and model serving infrastructure.

O projeto inclui uma solu√ß√£o completa de monitoramento usando **Prometheus** e **Grafana** para observabilidade da infraestrutura MLflow e model serving.

#### Quick Start

```bash
# Deploy monitoring stack
kubectl apply -f k8s/monitoring/prometheus-config.yaml
kubectl apply -f k8s/monitoring/prometheus-grafana-deployment.yaml

# Access Prometheus (port-forward)
kubectl port-forward svc/prometheus -n mlflow-prod 9090:9090
# ‚Üí http://127.0.0.1:9090

# Access Grafana (port-forward)
kubectl port-forward svc/grafana -n mlflow-prod 3000:3000
# ‚Üí http://127.0.0.1:3000 (admin / admin123456789)
```

#### Key Features

**Real-time Metrics:**
- MLflow server health and performance
- Model serving inference latency and throughput
- Kubernetes pod and node metrics
- HTTP request rates, errors, and latencies

**Pre-configured Dashboards:**
- MLflow & Model Server Monitoring (request rates, latencies, success rates)
- Kubernetes cluster health and resource utilization
- Custom PromQL queries for detailed analysis

**Alert Rules:**
- Critical: Server downtime, pod crashes, high error rates
- Warning: High CPU/memory usage, model latency spikes, training failures

#### Components

| Component | Port | Purpose |
|-----------|------|---------|
| **Prometheus** | 9090 (NodePort: 30090) | Metrics collection, storage, alerting rules |
| **Grafana** | 3000 (NodePort: 30300) | Metrics visualization, dashboards, alerts |

#### Scrape Targets

Prometheus automatically scrapes metrics from:
- Prometheus itself: `http://prometheus:9090`
- MLflow server: `http://mlflow-service:5000/metrics`
- Model server: `http://model-service:8000/metrics`
- Kubernetes API, nodes, and pods (service discovery)

#### Example Queries

**MLflow Request Health:**
```promql
sum(rate(http_requests_total{job="mlflow-server", status="200"}[5m])) by (path)
```

**Model Inference Latency (p95):**
```promql
histogram_quantile(0.95, rate(mlflow_model_request_duration_seconds_bucket[5m]))
```

**Pod CPU Usage:**
```promql
sum(rate(container_cpu_usage_seconds_total{namespace="mlflow-prod"}[5m])) by (pod)
```

#### For Detailed Configuration

See [monitoring/README.md](monitoring/README.md) for:
- Complete setup guide (Kubernetes, Minikube, cloud)
- Dasboard creation and customization
- PromQL query examples
- Troubleshooting
- Performance tuning
- Alert configuration

---

## üöÄ FastAPI REST API for Model Serving

This project includes a **production-ready FastAPI application** for serving machine learning model predictions via REST endpoints. The API is fully integrated with **Prometheus metrics**, **health checks**, and **batch prediction support**.

O projeto inclui uma **aplica√ß√£o FastAPI pronta para produ√ß√£o** para servir previs√µes de modelos de machine learning atrav√©s de endpoints REST. A API √© totalmente integrada com **m√©tricas Prometheus**, **health checks** e **suporte para previs√µes em lote**.

### Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         FastAPI Application (api.py)                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Endpoints:                                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - /health, /ping               [Health Checks]  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - /model/metadata, /features   [Model Info]     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - /predict                     [Single Pred]    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - /batch-predict               [Batch Pred]     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - /invocations                 [MLflow Compat]  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - /metrics, /prometheus-metrics[Prometheus]    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                       ‚Üì                                   ‚îÇ
‚îÇ          Load Model on Startup (artifacts/)              ‚îÇ
‚îÇ                       ‚Üì                                   ‚îÇ
‚îÇ         Pydantic Validation (schemas.py)                 ‚îÇ
‚îÇ                       ‚Üì                                   ‚îÇ
‚îÇ     Prometheus Metrics (counters, histograms, gauges)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Quick Start (Docker)

**Start the API server:**

```bash
# Start with Docker Compose
export DOCKER_API_VERSION=1.44
docker compose -f src/mlops_project/docker-compose.mlflow.yml up -d api

# The API will be available at http://127.0.0.1:8000
```

**Or run standalone (local):**

```bash
# Install FastAPI dependencies (included in requirements.txt)
pip install -r requirements.txt

# Run the API
python -m uvicorn src.mlops_project.api:app --host 0.0.0.0 --port 8000

# The API will be available at http://127.0.0.1:8000
```

### Available Endpoints

#### Health Checks & Info (Liveness/Readiness Probes)

| Method | Endpoint | Purpose | Status Code |
|--------|----------|---------|-------------|
| GET | `/health` | Kubernetes liveness probe | 200 if healthy |
| GET | `/ping` | MLflow compatibility check | 200 if ready |
| GET | `/model/metadata` | Model information (accuracy, F1, classes) | 200 |
| GET | `/model/features` | Expected feature names | 200 |

#### Predictions

| Method | Endpoint | Purpose | Input | Output |
|--------|----------|---------|-------|--------|
| POST | `/predict` | Single prediction | 30 features | Prediction + confidence |
| POST | `/batch-predict` | Batch predictions (1-1000) | Array of features | Array of predictions |
| POST | `/invocations` | MLflow-compatible endpoint | MLflow dataframe_split format | MLflow format response |

#### Monitoring & Metrics

| Method | Endpoint | Purpose |
|--------|----------|---------|
| GET | `/metrics` | Prometheus metrics (default format) |
| GET | `/prometheus-metrics` | Prometheus metrics (explicit) |
| GET | `/docs` | Interactive API documentation (Swagger UI) |
| GET | `/redoc` | Alternative API documentation (ReDoc) |

### Example Usage (cURL)

**1. Health Check**

```bash
curl -X GET http://127.0.0.1:8000/health
```

Response:
```json
{
  "status": "healthy",
  "version": "1.0.0",
  "model_loaded": true,
  "timestamp": "2024-01-15T10:30:45.123456"
}
```

**2. Get Model Metadata**

```bash
curl -X GET http://127.0.0.1:8000/model/metadata
```

Response:
```json
{
  "name": "sklearn-model",
  "version": "1.0.0",
  "accuracy": 0.92,
  "f1_score": 0.89,
  "classes": ["class_0", "class_1"],
  "n_features": 30
}
```

**3. Single Prediction**

```bash
curl -X POST http://127.0.0.1:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "feature_1": 0.5,
    "feature_2": 0.3,
    "feature_3": 0.7,
    ... (30 features total)
  }'
```

Response:
```json
{
  "prediction": 1,
  "probability": [0.08, 0.92],
  "confidence": 0.92,
  "timestamp": "2024-01-15T10:30:45.123456"
}
```

**4. Batch Predictions (Multiple Items)**

```bash
curl -X POST http://127.0.0.1:8000/batch-predict \
  -H "Content-Type: application/json" \
  -d '{
    "records": [
      {
        "feature_1": 0.5,
        "feature_2": 0.3,
        ... (30 features total)
      },
      {
        "feature_1": 0.2,
        "feature_2": 0.8,
        ... (30 features total)
      }
    ]
  }'
```

Response:
```json
{
  "predictions": [
    {
      "prediction": 1,
      "probability": [0.08, 0.92],
      "confidence": 0.92
    },
    {
      "prediction": 0,
      "probability": [0.78, 0.22],
      "confidence": 0.78
    }
  ],
  "processing_time_ms": 45.23,
  "total_records": 2
}
```

**5. Get Metrics (Prometheus format)**

```bash
curl -s http://127.0.0.1:8000/metrics | head -20
```

Output:
```
# HELP prediction_requests_total Total number of prediction requests
# TYPE prediction_requests_total counter
prediction_requests_total{endpoint="predict"} 127
prediction_requests_total{endpoint="batch_predict"} 34

# HELP prediction_latency_seconds Prediction processing latency
# TYPE prediction_latency_seconds histogram
prediction_latency_seconds_bucket{endpoint="predict",le="0.01"} 120
prediction_latency_seconds_bucket{endpoint="predict",le="0.05"} 125
prediction_latency_seconds_bucket{endpoint="predict",le="0.1"} 127
```

### Interactive API Documentation

Once the API is running, access the interactive documentation:

- **Swagger UI (Recommended):** http://127.0.0.1:8000/docs
- **ReDoc (Alternative):** http://127.0.0.1:8000/redoc

These interfaces allow you to:
- Browse all available endpoints
- View request/response schemas
- Test endpoints directly in the browser
- See real-time responses

### Deploying to Kubernetes

The project includes Kubernetes manifests for the FastAPI API:

```bash
# Deploy FastAPI API to Kubernetes
kubectl apply -f k8s/fastapi-deployment.yaml

# Check status
kubectl get all -n mlflow-prod

# Port-forward for local access
kubectl port-forward svc/fastapi-service -n mlflow-prod 8000:8000 &

# Access the API
curl http://localhost:8000/health
```

**Kubernetes Configuration Includes:**
- Deployment with auto-scaling (2-5 replicas)
- Service (NodePort: 30800 for external access)
- Health checks (liveness & readiness probes)
- Resource limits and requests
- Environment variables for configuration

### Monitoring the API (Prometheus + Grafana)

The FastAPI application exposes metrics compatible with Prometheus. Track:

- **Request volume:** `sum(rate(prediction_requests_total[5m]))`
- **Error rate:** `sum(rate(prediction_requests_failed[5m]))`
- **Latency (p95):** `histogram_quantile(0.95, rate(prediction_latency_seconds_bucket[5m]))`
- **Model load status:** `model_loaded` (gauge)
- **Uptime:** `api_uptime_seconds` (gauge)

**View metrics in Grafana:**
1. Access Grafana: http://127.0.0.1:3000 (port-forwarded)
2. Dashboard: "API Metrics" shows FastAPI performance
3. Alerts trigger if error rate > 5% or latency > 500ms

### API Configuration

**Environment Variables:**

| Variable | Default | Purpose |
|----------|---------|---------|
| `API_PORT` | 8000 | Port the API listens on |
| `MODEL_PATH` | `artifacts/model/model.joblib` | Path to trained model |
| `LOG_LEVEL` | `INFO` | Logging level (DEBUG, INFO, WARNING, ERROR) |
| `SERVE_API` | (set by docker-compose) | Enable API mode in Docker entrypoint |

**Model Requirements:**
- Must be a scikit-learn model (joblib-serialized)
- Must accept 30 input features
- Must support `.predict()` and `.predict_proba()` methods

### Error Handling

**400 Bad Request:** Invalid input (wrong number of features, missing fields)
```json
{
  "error": "Validation error",
  "details": "Expected 30 features, got 29"
}
```

**404 Not Found:** Model not loaded
```json
{
  "error": "Model not available",
  "details": "Model file not found at artifacts/model/model.joblib"
}
```

**500 Internal Server Error:** Server error
```json
{
  "error": "Internal server error",
  "details": "Model prediction failed"
}
```

### Performance Testing

```bash
# Test single prediction latency
time curl -X POST http://127.0.0.1:8000/predict \
  -H "Content-Type: application/json" \
  -d "@sample_request.json"

# Load test with Apache Bench
ab -n 1000 -c 10 -p sample_request.json \
  -T application/json http://127.0.0.1:8000/predict

# Load test with wrk (concurrent connections)
wrk -t4 -c100 -d30s -s test_predict.lua http://127.0.0.1:8000/predict
```

### For Detailed Information

See [src/mlops_project/](src/mlops_project/) for:
- `api.py` - FastAPI application with all endpoints
- `schemas.py` - Pydantic request/response models
- `docker-compose.mlflow.yml` - Docker Compose configuration with 'api' service
- `docker-entrypoint.sh` - Smart entrypoint (SERVE_API flag support)

---

## ‚úÖ Boas Pr√°ticas / Best Practices
- Estrutura modular e escal√°vel / Modular and scalable structure  
- Versionamento limpo com `.gitignore` / Clean versioning with `.gitignore`  
- Registro completo de experimentos com MLflow / Complete experiment tracking with MLflow  
- Separa√ß√£o clara entre **ETL, treinamento, avalia√ß√£o e deployment** / Clear separation of **ETL, training, evaluation, and deployment**  
- Documenta√ß√£o t√©cnica voltada para recrutadores / Technical documentation tailored for recruiters  

---

## üîÆ Extens√µes Futuras / Future Extensions
- Integra√ß√£o com **Docker/Kubernetes** / Integration with **Docker/Kubernetes**  
- Automa√ß√£o de pipeline com **CI/CD (GitHub Actions)** / Pipeline automation with **CI/CD (GitHub Actions)**  
- Monitoramento de modelos em produ√ß√£o / Model monitoring in production  
- Inclus√£o de testes unit√°rios e integra√ß√£o cont√≠nua / Unit testing and continuous integration  

---

## üìå Conclus√£o / Conclusion
Este projeto exemplifica um pipeline moderno de **MLOps**, destacando:  
This project exemplifies a modern **MLOps pipeline**, highlighting:  
- **Rigor t√©cnico / Technical rigor**  
- **Boas pr√°ticas de engenharia / Engineering best practices**  
- **Capacidade de deployment real / Real deployment capability**  

üëâ Ele serve como uma vitrine para recrutadores, demonstrando experi√™ncia pr√°tica em **Machine Learning aplicado a produ√ß√£o**.  
üëâ It serves as a showcase for recruiters, demonstrating practical experience in **Machine Learning applied to production**.

---
